import os
import json
import traceback
import uuid
import numpy as np
import logging
from dataclasses import dataclass, field  # Import field
from typing import Optional
from datetime import datetime, timezone  # Import datetime and timezone
from PIL import Image
# from PIL.PngImagePlugin import PngInfo # No longer needed for JPEG saving

from . import settings  # Import settings to get paths


# Queue file path (from settings)
QUEUE_FILE = settings.QUEUE_FILE_PATH

# Temp directory for queue images (from settings)
temp_queue_images = settings.TEMP_QUEUE_IMAGES_DIR
# os.makedirs(temp_queue_images, exist_ok=True) # Directory creation handled in settings.py


@dataclass
class QueuedJob:
    prompt: str
    image_path: str
    video_length: float
    job_id: str  # Changed to string for hex ID
    seed: int
    use_teacache: bool
    gpu_memory_preservation: float
    steps: int
    cfg: float
    gs: float
    rs: float
    status: str = "pending"
    thumbnail: str = ""
    mp4_crf: float = 16
    # Progress tracking fields
    progress: float = 0.0
    progress_step: int = 0
    progress_total: int = 0  # Default to 0, will be set by worker
    progress_info: str = ""
    lora_scale: float = 1.0  # 追加: LoRA強度
    lora_path: Optional[str] = None
    # Add updated_at timestamp, default to current UTC time
    updated_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    # Add field for original Exif data (bytes) - will not be saved in JSON
    original_exif: Optional[bytes] = field(default=None, repr=False)

    def to_dict(self):
        # Exclude original_exif from the dictionary saved to JSON
        try:
            # Convert datetime to ISO 8601 string format for JSON serialization
            updated_at_iso = self.updated_at.isoformat() if self.updated_at else None
            return {
                'prompt': self.prompt,
                'image_path': self.image_path,
                'video_length': self.video_length,
                'job_id': self.job_id,
                'seed': self.seed,
                'use_teacache': self.use_teacache,
                'gpu_memory_preservation': self.gpu_memory_preservation,
                'steps': self.steps,
                'cfg': self.cfg,
                'gs': self.gs,
                'rs': self.rs,
                'status': self.status,
                'thumbnail': self.thumbnail,
                'mp4_crf': self.mp4_crf,
                # Progress fields
                'progress': self.progress,
                'progress_step': self.progress_step,
                'progress_total': self.progress_total,
                'progress_info': self.progress_info,
                'lora_scale': self.lora_scale,  # 追加
                'lora_path': self.lora_path,
                'updated_at': updated_at_iso,  # Add updated_at
            }
        except Exception as e:
            print(f"Error converting job to dict: {str(e)}")
            return None

    @classmethod
    def from_dict(cls, data):
        try:
            # Convert ISO 8601 string back to datetime object
            updated_at_iso = data.get('updated_at')
            updated_at_dt = None
            if updated_at_iso:
                try:
                    # Handle potential 'Z' suffix for UTC
                    if updated_at_iso.endswith('Z'):
                        updated_at_iso = updated_at_iso[:-1] + '+00:00'
                    updated_at_dt = datetime.fromisoformat(updated_at_iso)
                    # Ensure timezone-aware (assume UTC if no timezone info)
                    if updated_at_dt.tzinfo is None:
                        updated_at_dt = updated_at_dt.replace(tzinfo=timezone.utc)  # Corrected indentation and comment space
                except ValueError:
                    print(f"Warning: Could not parse updated_at timestamp '{updated_at_iso}'. Using current time.")
                    updated_at_dt = datetime.now(timezone.utc)
            else:
                # If updated_at is missing, default to now
                updated_at_dt = datetime.now(timezone.utc)

            return cls(
                prompt=data.get('prompt', 'A character doing some simple body movements.'),
                image_path=data.get('image_path', ''),
                video_length=data.get('video_length', 5.0),
                job_id=data.get('job_id', uuid.uuid4().hex[:8]),  # Provide default if missing
                seed=data.get('seed', -1),
                use_teacache=data.get('use_teacache', True),  # Default to True
                gpu_memory_preservation=data.get('gpu_memory_preservation', 0.0),
                steps=data.get('steps', 20),
                cfg=data.get('cfg', 7.0),
                gs=data.get('gs', 1.0),
                rs=data.get('rs', 1.0),
                status=data.get('status', 'pending'),
                thumbnail=data.get('thumbnail', ''),
                mp4_crf=data.get('mp4_crf', 16.0),
                # Progress fields with defaults
                progress=data.get('progress', 0.0),
                progress_step=data.get('progress_step', 0),
                progress_total=data.get('progress_total', 0),
                progress_info=data.get('progress_info', ''),
                lora_scale=data.get('lora_scale', 1.0),   # 追加
                lora_path=data.get('lora_path', None),
                updated_at=updated_at_dt  # Add updated_at
            )
        except Exception as e:
            print(f"Error creating job from dict: {str(e)}")
            traceback.print_exc()
            return None


# Initialize job queue as a list
job_queue = []


def save_queue():
    global job_queue
    try:
        jobs = []
        for job in job_queue:
            job_dict = job.to_dict()
            if job_dict is not None:
                jobs.append(job_dict)

        file_path = os.path.abspath(QUEUE_FILE)
        with open(file_path, 'w') as f:
            json.dump(jobs, f, indent=2)
        return True
    except Exception as e:
        print(f"Error saving queue: {str(e)}")
        traceback.print_exc()
        return False


def load_queue_from_file() -> list[QueuedJob]:
    """Loads the queue from the JSON file and returns it as a list."""
    try:
        if os.path.exists(QUEUE_FILE):
            with open(QUEUE_FILE, 'r') as f:
                jobs_data = json.load(f)
            loaded_queue = []
            for job_data in jobs_data:
                job = QueuedJob.from_dict(job_data)
                if job is not None:
                    loaded_queue.append(job)
            return loaded_queue
        return []
    except Exception as e:
        print(f"Error loading queue: {str(e)}")
        traceback.print_exc()
        return []


# Load existing queue on startup into the global variable
job_queue = load_queue_from_file()


def save_image_to_temp(image: np.ndarray, job_id: str, prompt: str, seed: int, exif_data: Optional[bytes] = None) -> str:
    """Save image to temp directory as JPEG with Exif metadata and return the path"""
    try:
        # Convert numpy array to PIL Image
        squeezed_image = np.squeeze(image)
        pil_image = Image.fromarray(squeezed_image)
        # logging.info(f"[Job {job_id}] Exif in pil_image after fromarray: {pil_image.info.get('exif') is not None}") # DEBUG: Removed

        # Create unique filename using hex ID, change extension to jpg
        filename = f"queue_image_{job_id}.jpg"
        filepath = os.path.join(temp_queue_images, filename)

        # Prepare save arguments
        save_kwargs = {
            "format": "JPEG",
            "quality": 70,  # Lower quality for smaller file size
        }
        if exif_data:
            save_kwargs["exif"] = exif_data
            logging.info(f"[Job {job_id}] Attempting to save with Exif data.")
        else:
            logging.info(f"[Job {job_id}] No Exif data provided for saving.")

        # Save image as JPEG with or without Exif
        pil_image.save(filepath, **save_kwargs)
        # logging.info(f"[Job {job_id}] Saved temp image to {filepath} (JPEG)") # DEBUG: Removed
        return filepath
    except Exception as e:
        print(f"Error saving image with metadata: {str(e)}")
        traceback.print_exc()
        return ""


def add_to_queue(prompt, image, original_exif: Optional[bytes], video_length, seed, use_teacache, gpu_memory_preservation, steps, cfg, gs, rs, status="pending", mp4_crf=16, lora_scale: float = 1.0, lora_path: Optional[str] = None):
    global job_queue
    try:
        # Generate a unique hex ID for the job
        job_id = uuid.uuid4().hex[:8]
        # Save image to temp directory and get path
        image_array = np.array(image)
        # Pass original_exif to save_image_to_temp
        image_path = save_image_to_temp(image_array, job_id, prompt, seed, exif_data=original_exif)
        if not image_path:
            print("Failed to save image with Exif to temp, cannot add job.")
            return None

        job = QueuedJob(
            prompt=prompt,
            image_path=image_path,
            video_length=video_length,
            job_id=job_id,
            seed=seed,
            use_teacache=use_teacache,
            gpu_memory_preservation=gpu_memory_preservation,
            steps=steps,
            cfg=cfg,
            gs=gs,
            rs=rs,
            status=status,
            mp4_crf=mp4_crf,
            lora_scale=lora_scale,
            lora_path=lora_path,
            original_exif=original_exif  # Store exif in job object (won't be saved to JSON)
        )
        job_queue.append(job)
        if not save_queue():  # Save immediately after adding
            print("Failed to save queue after adding job.")
            # Optionally remove the job if saving fails, or handle differently
            # job_queue.pop()
            # return None
        return job_id
    except Exception as e:
        print(f"Error adding job to queue: {str(e)}")
        traceback.print_exc()
        return None


# Removed redundant traceback import


def get_next_job():
    """
    Gets the next pending job from the global in-memory queue,
    updates its status to 'processing' in the file, and returns the job object.
    Does NOT remove the job from the file until finalized.
    """
    global job_queue
    try:
        # Find the first 'pending' job in the in-memory queue
        job_to_process = None
        job_index = -1
        # Iterate through a copy of the queue indices to allow safe removal
        indices = list(range(len(job_queue)))
        for i in indices:
            # Check if index is still valid after potential removals by other threads/processes (unlikely here but safer)
            if i < len(job_queue):
                job = job_queue[i]
                if job.status == 'pending':
                    job_to_process = job
                    job_index = i
                    break  # Found the first pending job

        if job_to_process:
            # Remove from in-memory queue to prevent other workers taking it
            # Use del with index for potentially better performance than pop(index) in some scenarios
            del job_queue[job_index]
            logging.info(f"Worker picked up job {job_to_process.job_id}. Removed from in-memory pending list.")

            # Update status to 'processing' in the persistent queue file
            # We need to reload the queue from file, update the specific job, and save again.
            # This ensures we are working with the most current state from the file.
            current_persistent_queue = load_queue_from_file()
            job_found_in_file = False
            for job_in_file in current_persistent_queue:
                if job_in_file.job_id == job_to_process.job_id:
                    # Only update if it's still pending or somehow back to pending in the file
                    # If it's already processing/completed/failed by another interaction, log it.
                    if job_in_file.status == 'pending':
                        job_in_file.status = 'processing'
                        job_found_in_file = True
                        logging.info(f"Updating status to 'processing' for job {job_to_process.job_id} in the queue file.")
                    else:
                        # This might indicate a race condition or unexpected state.
                        logging.warning(f"Job {job_to_process.job_id} found in file but status is already '{job_in_file.status}', not 'pending'. Proceeding cautiously.")
                        # We still removed it from memory, so let the worker process it,
                        # but the file state might be inconsistent. Mark as found.
                        job_found_in_file = True  # Treat as found for saving logic below
                    break

            if job_found_in_file:
                # Save the updated queue back to the file
                try:
                    jobs_to_save = [j.to_dict() for j in current_persistent_queue if j.to_dict() is not None]
                    file_path = os.path.abspath(QUEUE_FILE)
                    with open(file_path, 'w') as f:
                        json.dump(jobs_to_save, f, indent=2)
                    logging.info(f"Queue file saved with job {job_to_process.job_id} marked as 'processing' (or existing status).")
                    # Return the job object that the worker will process
                    # Ensure the returned object also has the 'processing' status for the worker
                    job_to_process.status = 'processing'  # Set status for the object being returned
                    return job_to_process
                except Exception as e:
                    logging.error(f"Error saving queue after marking job {job_to_process.job_id} as processing: {e}")
                    traceback.print_exc()
                    # If saving fails, put the job back into the in-memory queue at the beginning
                    job_queue.insert(0, job_to_process)  # Re-insert at the beginning
                    logging.info(f"Re-inserted job {job_to_process.job_id} into memory queue due to save failure.")
                    return None
            else:
                # This case should ideally not happen if the job was just in the in-memory queue
                # loaded from the file, but handle defensively.
                logging.error(f"Job {job_to_process.job_id} was popped from memory but not found in the file for status update. This indicates a potential state inconsistency.")
                # Do not re-insert into memory queue as the file state is unknown/inconsistent.
                return None
        else:
            # No pending jobs found in the in-memory queue
            # logging.info("No pending jobs found in the in-memory queue.") # Optional: reduce log noise
            return None
    except Exception as e:
        logging.error(f"Error getting next job: {str(e)}")
        traceback.print_exc()
        return None


def get_job_by_id(job_id: str) -> QueuedJob | None:
    """Finds a job by its ID by reading the queue file directly."""
    logging.info(f"Attempting to get job by ID: {job_id}")
    current_queue = load_queue_from_file()  # Always read from file for this check
    for job in current_queue:
        if job.job_id == job_id:
            logging.info(f"Job {job_id} found. Progress: {job.progress}, Info: '{job.progress_info}'")
            return job
    logging.warning(f"Job {job_id} not found in queue file.")
    return None


def update_job_status(job_id: str, status: str, thumbnail: str = None):
    """Updates the status (and optionally thumbnail) of a job in the global queue and saves the file."""
    global job_queue
    job_updated = False  # Changed initial value to False
    job_found_in_memory = False
    for job in job_queue:
        if job.job_id == job_id:
            # Update status and timestamp
            if job.status != status:  # Only update timestamp if status actually changes
                job.status = status
                job.updated_at = datetime.now(timezone.utc)
                job_updated = True
            if thumbnail and job.thumbnail != thumbnail:
                job.thumbnail = thumbnail
                job.updated_at = datetime.now(timezone.utc)  # Also update if thumbnail changes
                job_updated = True  # Mark as updated if thumbnail changed

            if job_updated:
                job_found_in_memory = True
            break

    if job_updated:
        save_queue()  # Save if updated in memory

    # If not found or updated in memory, try loading from file, updating, and saving
    if not job_found_in_memory:
        current_queue = load_queue_from_file()
        job_found_in_file = False
        for job in current_queue:
            if job.job_id == job_id:
                # Update status and timestamp in file data
                if job.status != status:
                    job.status = status
                    job.updated_at = datetime.now(timezone.utc)
                    job_found_in_file = True
                if thumbnail and job.thumbnail != thumbnail:
                    job.thumbnail = thumbnail
                    job.updated_at = datetime.now(timezone.utc)
                    job_found_in_file = True  # Mark as found if thumbnail changed

                if job_found_in_file:
                    break

        if job_found_in_file:
            # Overwrite the file with the updated list
            try:
                jobs_to_save = [j.to_dict() for j in current_queue if j.to_dict() is not None]
                file_path = os.path.abspath(QUEUE_FILE)
                with open(file_path, 'w') as f:
                    json.dump(jobs_to_save, f, indent=2)
                job_updated = True  # Mark as updated since we saved the file
                # Update the global in-memory queue as well
                job_queue = current_queue  # Update global variable after successful save
            except Exception as e:
                print(f"Error saving queue after updating job {job_id} found in file: {e}")
                traceback.print_exc()
                job_updated = False  # Ensure update status reflects save failure
        else:
            print(f"Job with ID {job_id} not found in memory or file for status update.")

    return job_updated


# Configure logging (moved import to top)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def update_job_progress(job_id: str, progress: float, step: int, total: int, info: str):
    """Updates the progress fields of a job in the global queue and saves the file."""
    logging.info(f"Attempting to update progress for job {job_id}: progress={progress}, step={step}, total={total}, info='{info}'")
    global job_queue
    job_updated = False
    job_found_in_memory = False
    for job in job_queue:
        if job.job_id == job_id:
            # Update progress fields and timestamp
            needs_update = False
            if job.progress != progress:
                job.progress = progress
                needs_update = True
            if job.progress_step != step:
                job.progress_step = step
                needs_update = True
            if job.progress_total != total:
                job.progress_total = total
                needs_update = True
            if job.progress_info != info:
                job.progress_info = info
                needs_update = True

            if needs_update:
                job.updated_at = datetime.now(timezone.utc)
                job_updated = True
                job_found_in_memory = True
                logging.info(f"Job {job_id} found in memory. Updating progress and timestamp.")
            break

    if job_updated:
        logging.info(f"Progress updated for job {job_id} in memory. Attempting to save queue.")
        if save_queue():  # Save if updated in memory
            logging.info(f"Queue saved successfully after updating progress for job {job_id} in memory.")
        else:
            logging.error(f"Failed to save queue after updating progress for job {job_id} in memory.")

    # If not found or updated in memory, try loading from file, updating, and saving
    if not job_found_in_memory:
        logging.info(f"Job {job_id} not found in memory. Attempting to load from file.")
        current_queue = load_queue_from_file()
        job_found_in_file = False
        for job in current_queue:
            if job.job_id == job_id:
                logging.info(f"Job {job_id} found in file. Updating progress.")
                # Update progress fields and timestamp in file data
                needs_update = False
                if job.progress != progress:
                    job.progress = progress
                    needs_update = True
                if job.progress_step != step:
                    job.progress_step = step
                    needs_update = True
                if job.progress_total != total:
                    job.progress_total = total
                    needs_update = True
                if job.progress_info != info:
                    job.progress_info = info
                    needs_update = True

                if needs_update:
                    job.updated_at = datetime.now(timezone.utc)
                    job_found_in_file = True
                    logging.info(f"Job {job_id} found in file. Updating progress and timestamp.")
                break

        if job_found_in_file:
            logging.info(f"Progress updated for job {job_id} in file data. Attempting to save queue.")
            # Overwrite the file with the updated list
            try:
                jobs_to_save = [j.to_dict() for j in current_queue if j.to_dict() is not None]
                file_path = os.path.abspath(QUEUE_FILE)
                with open(file_path, 'w') as f:
                    json.dump(jobs_to_save, f, indent=2)
                job_updated = True  # Mark as updated since we saved the file
                logging.info(f"Queue saved successfully after updating progress for job {job_id} found in file.")
                # Update the global in-memory queue as well
                job_queue = current_queue  # Update global variable after successful save
                logging.info(f"Global job_queue updated with file content for job {job_id}.")
            except Exception as e:
                logging.error(f"Error saving queue after updating progress for job {job_id} found in file: {e}")
                traceback.print_exc()
                job_updated = False  # Ensure update status reflects save failure
        else:
            logging.warning(f"Job with ID {job_id} not found in memory or file for progress update.")

    if not job_updated:
        logging.warning(f"Progress update failed for job {job_id}.")
    return job_updated


def get_queue_status():
    """Returns a list of job statuses and basic info."""
    global job_queue
    # Load the latest queue to ensure we have the most recent list in memory
    # Note: This might overwrite changes made by other processes if not careful,
    # but it's necessary for get_queue_status to be accurate.
    # Consider locking mechanisms for multi-worker scenarios.
    load_queue_from_file()  # Reload global job_queue
    status_list = []
    for job in job_queue:  # Iterate over the reloaded global queue
        logging.info(f"Getting status for job {job.job_id}. Current progress: {job.progress}")  # Log progress here
        status_list.append({
            "job_id": job.job_id,
            "status": job.status,
            "prompt": job.prompt[:50] + "..." if len(job.prompt) > 50 else job.prompt,  # Truncate long prompts
            "video_length": job.video_length,
            # Include progress in status summary
            "progress": job.progress,
            "progress_info": job.progress_info,
        })
    return status_list

# --- Functions below might be Gradio specific and potentially moved later or adapted ---


def update_queue_display():
    """
    This function seems tightly coupled with Gradio's display logic.
    For the API, we might need a different way to represent the queue,
    perhaps just returning the list of job dicts or a summary.
    Keeping it here for now, but marked for review.
    """
    global job_queue
    try:
        # Reload queue to ensure it's current
        load_queue_from_file()  # Reload global job_queue
        queue_data = []
        for job in job_queue:  # Iterate over the reloaded global queue
            # Create thumbnail if it doesn't exist and image path is valid
            if not job.thumbnail and job.image_path and os.path.exists(job.image_path):
                try:
                    # Load and resize image for thumbnail
                    img = Image.open(job.image_path)
                    width, height = img.size
                    new_height = 100  # Smaller thumbnail for potentially long queues
                    new_width = int((new_height / height) * width)
                    img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                    thumb_path = os.path.join(temp_queue_images, f"thumb_{job.job_id}.png")
                    img.save(thumb_path)
                    job.thumbnail = thumb_path
                    save_queue()  # Save queue after updating thumbnail path
                except FileNotFoundError:
                    print(f"Image file not found for job {job.job_id} at path: {job.image_path}. Skipping thumbnail generation.")
                    job.thumbnail = ""  # Ensure thumbnail is empty if image is missing
                except Exception as e:
                    print(f"Error creating thumbnail for job {job.job_id}: {str(e)}")
                    job.thumbnail = ""  # Ensure thumbnail is empty on error
            elif not os.path.exists(job.image_path):
                print(f"Image file not found for job {job.job_id} at path: {job.image_path}. Skipping thumbnail display.")
                job.thumbnail = ""  # Ensure thumbnail is empty if image is missing

            # Add job data to display if thumbnail exists
            if job.thumbnail and os.path.exists(job.thumbnail):
                caption = f"ID: {job.job_id}\nStatus: {job.status}\nLen: {job.video_length}s\nPrompt: {job.prompt[:30]}..."
                queue_data.append((job.thumbnail, caption))
            # Optionally, add placeholder if no thumbnail
            # else:
            #     caption = f"ID: {job.job_id}\nStatus: {job.status}\nLen: {job.video_length}s\nPrompt: {job.prompt[:30]}...\n(No Thumbnail)"
            #     # You might need a placeholder image path or handle this differently in Gradio
            #     placeholder_thumb = "path/to/placeholder.png" # Define a placeholder image
            #     if os.path.exists(placeholder_thumb):
        # End of the try block from line 523
        return queue_data  # Return data collected in the try block
    except Exception as e:  # Add except block for the try at line 523
        print(f"Error updating queue display: {str(e)}")
        # traceback.print_exc()  # Consider removing or using logging for production
        return []  # Return empty list on error


# --- Cleanup Function ---


def cleanup_jobs_by_max_count(max_completed_jobs: int = settings.MAX_COMPLETED_JOBS):
    """
    Removes old completed, cancelled, or failed jobs if their total count exceeds the limit.
    Jobs are removed based on their 'updated_at' timestamp (oldest first).
    Also removes associated temporary files (input image, thumbnail).
    """
    global job_queue
    logging.info(f"Starting job cleanup. Max completed/cancelled/failed jobs to keep: {max_completed_jobs}")

    try:
        current_queue = load_queue_from_file()
        # terminal_statuses = {"completed", "cancelled", "failed"} # Unused variable

        # Separate jobs by status
        active_jobs = []  # pending, processing
        terminal_jobs = []  # completed, cancelled, failed

        for job in current_queue:
            # Handle potential variations in 'failed' status (e.g., "failed - Reason")
            is_terminal = job.status == "completed" or job.status == "cancelled" or job.status.startswith("failed")
            if is_terminal:
                terminal_jobs.append(job)
            else:
                active_jobs.append(job)

        num_terminal_jobs = len(terminal_jobs)
        logging.info(f"Found {num_terminal_jobs} terminal jobs (completed/cancelled/failed).")

        if num_terminal_jobs <= max_completed_jobs:
            logging.info("Number of terminal jobs does not exceed the limit. No cleanup needed.")
            return 0  # No jobs removed

        # Sort terminal jobs by updated_at timestamp (oldest first)
        # Handle potential None values for updated_at just in case
        terminal_jobs.sort(key=lambda j: j.updated_at or datetime.min.replace(tzinfo=timezone.utc))

        # Determine how many jobs to remove
        num_to_remove = num_terminal_jobs - max_completed_jobs
        jobs_to_remove = terminal_jobs[:num_to_remove]
        jobs_to_keep = terminal_jobs[num_to_remove:]

        logging.info(f"Exceeded limit by {num_to_remove} jobs. Preparing to remove oldest ones.")

        removed_count = 0
        files_to_delete = []

        # Identify files associated with jobs to be removed
        for job in jobs_to_remove:
            logging.info(f"Marking job {job.job_id} (status: {job.status}, updated: {job.updated_at}) for removal.")
            if job.image_path:
                files_to_delete.append(job.image_path)
            if job.thumbnail:
                files_to_delete.append(job.thumbnail)
            removed_count += 1

        # Combine kept terminal jobs and active jobs for the new queue
        new_queue_jobs = active_jobs + jobs_to_keep

        # Save the new queue to the file
        try:
            jobs_to_save = [j.to_dict() for j in new_queue_jobs if j.to_dict() is not None]
            file_path = os.path.abspath(QUEUE_FILE)
            with open(file_path, 'w') as f:
                json.dump(jobs_to_save, f, indent=2)
            logging.info(f"Successfully saved updated queue file with {len(new_queue_jobs)} jobs.")
            # Update the global in-memory queue as well
            job_queue = new_queue_jobs
        except Exception as e:
            logging.error(f"Error saving queue file during cleanup: {e}")
            traceback.print_exc()
            # If saving fails, we should not proceed with file deletion
            return 0

        # Delete associated files *after* successfully saving the queue
        deleted_files_count = 0
        for file_path in files_to_delete:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    logging.info(f"Deleted temporary file: {file_path}")
                    deleted_files_count += 1
                else:
                    logging.warning(f"Temporary file not found for deletion: {file_path}")
            except Exception as e:
                logging.error(f"Error deleting temporary file {file_path}: {e}")

        logging.info(f"Job cleanup finished. Removed {removed_count} job entries and attempted to delete {deleted_files_count} associated files.")
        return removed_count

    except Exception as e:
        logging.error(f"An unexpected error occurred during job cleanup: {e}")
        traceback.print_exc()
        return 0
# Removed erroneous code from previous diff attempts
