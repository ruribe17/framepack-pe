import os
import json
import traceback
import uuid
import numpy as np
from dataclasses import dataclass
from PIL import Image

from . import settings  # Import settings to get paths

# Queue file path (from settings)
QUEUE_FILE = settings.QUEUE_FILE_PATH

# Temp directory for queue images (from settings)
temp_queue_images = settings.TEMP_QUEUE_IMAGES_DIR
# os.makedirs(temp_queue_images, exist_ok=True) # Directory creation handled in settings.py


@dataclass
class QueuedJob:
    prompt: str
    image_path: str
    video_length: float
    job_id: str  # Changed to string for hex ID
    seed: int
    use_teacache: bool
    gpu_memory_preservation: float
    steps: int
    cfg: float
    gs: float
    rs: float
    status: str = "pending"
    thumbnail: str = ""
    mp4_crf: float = 16
    # Progress tracking fields
    progress: float = 0.0
    progress_step: int = 0
    progress_total: int = 0  # Default to 0, will be set by worker
    progress_info: str = ""

    def to_dict(self):
        try:
            return {
                'prompt': self.prompt,
                'image_path': self.image_path,
                'video_length': self.video_length,
                'job_id': self.job_id,
                'seed': self.seed,
                'use_teacache': self.use_teacache,
                'gpu_memory_preservation': self.gpu_memory_preservation,
                'steps': self.steps,
                'cfg': self.cfg,
                'gs': self.gs,
                'rs': self.rs,
                'status': self.status,
                'thumbnail': self.thumbnail,
                'mp4_crf': self.mp4_crf,
                # Progress fields
                'progress': self.progress,
                'progress_step': self.progress_step,
                'progress_total': self.progress_total,
                'progress_info': self.progress_info,
            }
        except Exception as e:
            print(f"Error converting job to dict: {str(e)}")
            return None

    @classmethod
    def from_dict(cls, data):
        try:
            return cls(
                prompt=data.get('prompt', 'A character doing some simple body movements.'),  # Set default prompt
                image_path=data.get('image_path', ''),
                video_length=data.get('video_length', 5.0),
                job_id=data.get('job_id', uuid.uuid4().hex[:8]),  # Provide default if missing
                seed=data.get('seed', -1),
                use_teacache=data.get('use_teacache', True),  # Default to True
                gpu_memory_preservation=data.get('gpu_memory_preservation', 0.0),
                steps=data.get('steps', 20),
                cfg=data.get('cfg', 7.0),
                gs=data.get('gs', 1.0),
                rs=data.get('rs', 1.0),
                status=data.get('status', 'pending'),
                thumbnail=data.get('thumbnail', ''),
                mp4_crf=data.get('mp4_crf', 16.0),
                # Progress fields with defaults
                progress=data.get('progress', 0.0),
                progress_step=data.get('progress_step', 0),
                progress_total=data.get('progress_total', 0),
                progress_info=data.get('progress_info', '')
            )
        except Exception as e:
            print(f"Error creating job from dict: {str(e)}")
            traceback.print_exc()
            return None


# Initialize job queue as a list
job_queue = []


def save_queue():
    global job_queue
    try:
        jobs = []
        for job in job_queue:
            job_dict = job.to_dict()
            if job_dict is not None:
                jobs.append(job_dict)

        file_path = os.path.abspath(QUEUE_FILE)
        with open(file_path, 'w') as f:
            json.dump(jobs, f, indent=2)
        return True
    except Exception as e:
        print(f"Error saving queue: {str(e)}")
        traceback.print_exc()
        return False


def load_queue_from_file() -> list[QueuedJob]:
    """Loads the queue from the JSON file and returns it as a list."""
    try:
        if os.path.exists(QUEUE_FILE):
            with open(QUEUE_FILE, 'r') as f:
                jobs_data = json.load(f)
            loaded_queue = []
            for job_data in jobs_data:
                job = QueuedJob.from_dict(job_data)
                if job is not None:
                    loaded_queue.append(job)
            return loaded_queue
        return []
    except Exception as e:
        print(f"Error loading queue: {str(e)}")
        traceback.print_exc()
        return []


# Load existing queue on startup into the global variable
job_queue = load_queue_from_file()


def save_image_to_temp(image: np.ndarray, job_id: str) -> str:
    """Save image to temp directory and return the path"""
    try:
        # Convert numpy array to PIL Image
        # Remove single-dimensional entries from the shape of an array
        squeezed_image = np.squeeze(image)
        pil_image = Image.fromarray(squeezed_image)
        # Create unique filename using hex ID
        filename = f"queue_image_{job_id}.png"
        filepath = os.path.join(temp_queue_images, filename)
        # Save image
        pil_image.save(filepath)
        return filepath
    except Exception as e:
        print(f"Error saving image: {str(e)}")
        traceback.print_exc()
        return ""


def add_to_queue(prompt, image, video_length, seed, use_teacache, gpu_memory_preservation, steps, cfg, gs, rs, status="pending", mp4_crf=16):
    global job_queue
    try:
        # Generate a unique hex ID for the job
        job_id = uuid.uuid4().hex[:8]
        # Save image to temp directory and get path
        image_array = np.array(image)
        image_path = save_image_to_temp(image_array, job_id)
        if not image_path:
            print("Failed to save image to temp, cannot add job.")
            return None

        job = QueuedJob(
            prompt=prompt,
            image_path=image_path,
            video_length=video_length,
            job_id=job_id,
            seed=seed,
            use_teacache=use_teacache,
            gpu_memory_preservation=gpu_memory_preservation,
            steps=steps,
            cfg=cfg,
            gs=gs,
            rs=rs,
            status=status,
            mp4_crf=mp4_crf
        )
        job_queue.append(job)
        if not save_queue():  # Save immediately after adding
            print("Failed to save queue after adding job.")
            # Optionally remove the job if saving fails, or handle differently
            # job_queue.pop()
            # return None
        return job_id
    except Exception as e:
        print(f"Error adding job to queue: {str(e)}")
        traceback.print_exc()
        return None


def get_next_job():
    """Gets the next job from the global in-memory queue, removes it, and saves the queue."""
    global job_queue
    try:
        if job_queue:
            job = job_queue.pop(0)  # Remove and return first job
            save_queue()  # Save after removing job
            return job
        return None
    except Exception as e:
        print(f"Error getting next job: {str(e)}")
        traceback.print_exc()
        return None


def get_job_by_id(job_id: str) -> QueuedJob | None:
    """Finds a job by its ID by reading the queue file directly."""
    logging.info(f"Attempting to get job by ID: {job_id}")
    current_queue = load_queue_from_file()  # Always read from file for this check
    for job in current_queue:
        if job.job_id == job_id:
            logging.info(f"Job {job_id} found. Progress: {job.progress}, Info: '{job.progress_info}'")
            return job
    logging.warning(f"Job {job_id} not found in queue file.")
    return None


def update_job_status(job_id: str, status: str, thumbnail: str = None):
    """Updates the status (and optionally thumbnail) of a job in the global queue and saves the file."""
    global job_queue
    job_updated = False  # Changed initial value to False
    job_found_in_memory = False
    for job in job_queue:
        if job.job_id == job_id:
            job.status = status
            if thumbnail:
                job.thumbnail = thumbnail
            job_updated = True
            job_found_in_memory = True
            break

    if job_updated:
        save_queue()  # Save if updated in memory

    # If not found or updated in memory, try loading from file, updating, and saving
    if not job_found_in_memory:
        current_queue = load_queue_from_file()
        job_found_in_file = False
        for job in current_queue:
            if job.job_id == job_id:
                job.status = status
                if thumbnail:
                    job.thumbnail = thumbnail
                job_found_in_file = True
                break

        if job_found_in_file:
            # Overwrite the file with the updated list
            try:
                jobs_to_save = [j.to_dict() for j in current_queue if j.to_dict() is not None]
                file_path = os.path.abspath(QUEUE_FILE)
                with open(file_path, 'w') as f:
                    json.dump(jobs_to_save, f, indent=2)
                job_updated = True  # Mark as updated since we saved the file
                # Update the global in-memory queue as well
                job_queue = current_queue  # Update global variable after successful save
            except Exception as e:
                print(f"Error saving queue after updating job {job_id} found in file: {e}")
                traceback.print_exc()
                job_updated = False  # Ensure update status reflects save failure
        else:
            print(f"Job with ID {job_id} not found in memory or file for status update.")

    return job_updated


import logging # Add logging import at the top if not already present

# Configure logging (add this near the top of the file)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def update_job_progress(job_id: str, progress: float, step: int, total: int, info: str):
    """Updates the progress fields of a job in the global queue and saves the file."""
    logging.info(f"Attempting to update progress for job {job_id}: progress={progress}, step={step}, total={total}, info='{info}'")
    global job_queue
    job_updated = False
    job_found_in_memory = False
    for job in job_queue:
        if job.job_id == job_id:
            job.progress = progress
            job.progress_step = step
            job.progress_total = total
            job.progress_info = info
            job_updated = True
            job_found_in_memory = True
            logging.info(f"Job {job_id} found in memory. Updating progress.")
            break

    if job_updated:
        logging.info(f"Progress updated for job {job_id} in memory. Attempting to save queue.")
        if save_queue(): # Save if updated in memory
             logging.info(f"Queue saved successfully after updating progress for job {job_id} in memory.")
        else:
             logging.error(f"Failed to save queue after updating progress for job {job_id} in memory.")

    # If not found or updated in memory, try loading from file, updating, and saving
    if not job_found_in_memory:
        logging.info(f"Job {job_id} not found in memory. Attempting to load from file.")
        current_queue = load_queue_from_file()
        job_found_in_file = False
        for job in current_queue:
            if job.job_id == job_id:
                logging.info(f"Job {job_id} found in file. Updating progress.")
                job.progress = progress
                job.progress_step = step
                job.progress_total = total
                job.progress_info = info
                job_found_in_file = True
                break

        if job_found_in_file:
            logging.info(f"Progress updated for job {job_id} in file data. Attempting to save queue.")
            # Overwrite the file with the updated list
            try:
                jobs_to_save = [j.to_dict() for j in current_queue if j.to_dict() is not None]
                file_path = os.path.abspath(QUEUE_FILE)
                with open(file_path, 'w') as f:
                    json.dump(jobs_to_save, f, indent=2)
                job_updated = True  # Mark as updated since we saved the file
                logging.info(f"Queue saved successfully after updating progress for job {job_id} found in file.")
                # Update the global in-memory queue as well
                job_queue = current_queue  # Update global variable after successful save
                logging.info(f"Global job_queue updated with file content for job {job_id}.")
            except Exception as e:
                logging.error(f"Error saving queue after updating progress for job {job_id} found in file: {e}")
                traceback.print_exc()
                job_updated = False  # Ensure update status reflects save failure
        else:
            logging.warning(f"Job with ID {job_id} not found in memory or file for progress update.")

    if not job_updated:
        logging.warning(f"Progress update failed for job {job_id}.")
    return job_updated


def get_queue_status():
    """Returns a list of job statuses and basic info."""
    global job_queue
    # Load the latest queue to ensure we have the most recent list in memory
    # Note: This might overwrite changes made by other processes if not careful,
    # but it's necessary for get_queue_status to be accurate.
    # Consider locking mechanisms for multi-worker scenarios.
    load_queue_from_file()  # Reload global job_queue
    status_list = []
    for job in job_queue:  # Iterate over the reloaded global queue
        logging.info(f"Getting status for job {job.job_id}. Current progress: {job.progress}") # Log progress here
        status_list.append({
            "job_id": job.job_id,
            "status": job.status,
            "prompt": job.prompt[:50] + "..." if len(job.prompt) > 50 else job.prompt,  # Truncate long prompts
            "video_length": job.video_length,
            # Include progress in status summary
            "progress": job.progress,
            "progress_info": job.progress_info,
        })
    return status_list

# --- Functions below might be Gradio specific and potentially moved later or adapted ---


def update_queue_display():
    """
    This function seems tightly coupled with Gradio's display logic.
    For the API, we might need a different way to represent the queue,
    perhaps just returning the list of job dicts or a summary.
    Keeping it here for now, but marked for review.
    """
    global job_queue
    try:
        # Reload queue to ensure it's current
        load_queue_from_file()  # Reload global job_queue
        queue_data = []
        for job in job_queue:  # Iterate over the reloaded global queue
            # Create thumbnail if it doesn't exist and image path is valid
            if not job.thumbnail and job.image_path and os.path.exists(job.image_path):
                try:
                    # Load and resize image for thumbnail
                    img = Image.open(job.image_path)
                    width, height = img.size
                    new_height = 100  # Smaller thumbnail for potentially long queues
                    new_width = int((new_height / height) * width)
                    img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                    thumb_path = os.path.join(temp_queue_images, f"thumb_{job.job_id}.png")
                    img.save(thumb_path)
                    job.thumbnail = thumb_path
                    save_queue()  # Save queue after updating thumbnail path
                except FileNotFoundError:
                    print(f"Image file not found for job {job.job_id} at path: {job.image_path}. Skipping thumbnail generation.")
                    job.thumbnail = ""  # Ensure thumbnail is empty if image is missing
                except Exception as e:
                    print(f"Error creating thumbnail for job {job.job_id}: {str(e)}")
                    job.thumbnail = ""  # Ensure thumbnail is empty on error
            elif not os.path.exists(job.image_path):
                print(f"Image file not found for job {job.job_id} at path: {job.image_path}. Skipping thumbnail display.")
                job.thumbnail = ""  # Ensure thumbnail is empty if image is missing

            # Add job data to display if thumbnail exists
            if job.thumbnail and os.path.exists(job.thumbnail):
                caption = f"ID: {job.job_id}\nStatus: {job.status}\nLen: {job.video_length}s\nPrompt: {job.prompt[:30]}..."
                queue_data.append((job.thumbnail, caption))
            # Optionally, add placeholder if no thumbnail
            # else:
            #     caption = f"ID: {job.job_id}\nStatus: {job.status}\nLen: {job.video_length}s\nPrompt: {job.prompt[:30]}...\n(No Thumbnail)"
            #     # You might need a placeholder image path or handle this differently in Gradio
            #     placeholder_thumb = "path/to/placeholder.png" # Define a placeholder image
            #     if os.path.exists(placeholder_thumb):
            #         queue_data.append((placeholder_thumb, caption))

        return queue_data
    except Exception as e:
        print(f"Error updating queue display: {str(e)}")
        traceback.print_exc()
        return []